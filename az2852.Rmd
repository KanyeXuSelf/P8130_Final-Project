---
title: "p8130_bm_final_proj"
output: github_document
date: "2024-11-25"
editor_options: 
  chunk_output_type: inline
---

```{r setup, message=FALSE}
library(tidyverse)
library(broom)
library(corrplot)
library(leaps)
library(glmnet)
library(faraway)
library(caret)
library(MASS) # boxcox
library(performance) # vif
```

```{r}
df = read_csv("data/Project_1_data.csv")

score_df = df %>% 
  janitor::clean_names() %>% 
  drop_na() %>% # Drop rows with any missing values
  mutate(across(where(is.character), as.factor)) %>% 
  mutate(
    parent_educ = fct_relevel(parent_educ, "bachelor's degree"),
    lunch_type = fct_relevel(lunch_type, "standard"),
    parent_marital_status = fct_relevel(parent_marital_status, "married"),
    practice_sport = fct_relevel(practice_sport, "never"),
    wkly_study_hours = fct_relevel(wkly_study_hours, "<5"),
    wkly_study_hours = fct_recode(wkly_study_hours, "5 to 10" = "10-May"),
    transport_means = fct_relevel( transport_means, "private")
  )
  

score_df

#create a df just for predicting math score
math_df = 
  score_df %>% 
  dplyr::select(-reading_score, -writing_score) %>% 
  dplyr::select(math_score, everything())

# convert data to numeric
math_df_numeric = math_df %>% 
  mutate(across(where(is.factor), ~ as.numeric(.)))
```

```{r}
# Descriptive Statistics
summary(score_df)

# Check for missing values
colSums(is.na(score_df))
```
No missing values in the outcome variables. significant missing in various predictor variables.
```{r}
# Visualize distributions of outcome variables
ggplot(score_df, aes(x = math_score)) + 
  geom_histogram(binwidth = 5, fill = "blue", alpha = 0.7) + 
  labs(title = "Distribution of Math Scores", x = "Math Score", y = "Frequency") +
  theme_minimal()

ggplot(score_df, aes(x = reading_score)) + 
  geom_histogram(binwidth = 5, fill = "green", alpha = 0.7) + 
  labs(title = "Distribution of Reading Scores", x = "Reading Score", y = "Frequency") +
  theme_minimal()

ggplot(score_df, aes(x = writing_score)) + 
  geom_histogram(binwidth = 5, fill = "red", alpha = 0.7) + 
  labs(title = "Distribution of Writing Scores", x = "Writing Score", y = "Frequency") +
  theme_minimal()
```
```{r}
par(mfrow=c(1,3))
boxplot(score_df$math_score, main='math_score')
boxplot(score_df$reading_score, main='reading_score')
boxplot(score_df$writing_score, main='writing_score')
```

```{r}
library(patchwork)

# Create individual bar plots for each categorical variable
plot_gender <- ggplot(score_df, aes(x = gender)) +
  geom_bar(fill = "skyblue") +
  labs(title = "Gender Distribution", x = "Gender", y = "Count") +
  theme_minimal()

plot_ethnic_group <- ggplot(score_df, aes(x = ethnic_group)) +
  geom_bar(fill = "lightgreen") +
  labs(title = "Ethnic Group Distribution", x = "Ethnic Group", y = "Count") +
  theme_minimal()

plot_parent_educ <- ggplot(score_df, aes(x = parent_educ)) +
  geom_bar(fill = "purple") +
  labs(title = "Parent Education Distribution", x = "Parent Education", y = "Count") +
  theme_minimal()

plot_lunch_type <- ggplot(score_df, aes(x = lunch_type)) +
  geom_bar(fill = "orange") +
  labs(title = "Lunch Type Distribution", x = "Lunch Type", y = "Count") +
  theme_minimal()

plot_test_prep <- ggplot(score_df, aes(x = test_prep)) +
  geom_bar(fill = "pink") +
  labs(title = "Test Prep Distribution", x = "Test Prep", y = "Count") +
  theme_minimal()

plot_practice_sport <- ggplot(score_df, aes(x = practice_sport)) +
  geom_bar(fill = "cyan") +
  labs(title = "Practice Sport Distribution", x = "Practice Sport", y = "Count") +
  theme_minimal()

# Example for other variables (add similar code for all remaining variables)
plot_is_first_child <- ggplot(score_df, aes(x = is_first_child)) +
  geom_bar(fill = "lightblue") +
  labs(title = "First Child Status Distribution", x = "Is First Child", y = "Count") +
  theme_minimal()

plot_nr_siblings <- ggplot(score_df, aes(x = factor(nr_siblings))) +
  geom_bar(fill = "red") +
  labs(title = "Number of Siblings Distribution", x = "Number of Siblings", y = "Count") +
  theme_minimal()

plot_transport_means <- ggplot(score_df, aes(x = transport_means)) +
  geom_bar(fill = "gold") +
  labs(title = "Transport Means Distribution", x = "Transport Means", y = "Count") +
  theme_minimal()

plot_wkly_study_hours <- ggplot(score_df, aes(x = wkly_study_hours)) +
  geom_bar(fill = "darkgreen") +
  labs(title = "Weekly Study Hours Distribution", x = "Weekly Study Hours", y = "Count") +
  theme_minimal()

# Adjusting plot layout with more vertical space
combined_plot <- (
  plot_gender + plot_ethnic_group + plot_parent_educ +
  plot_lunch_type + plot_test_prep + plot_practice_sport +
  plot_is_first_child + plot_nr_siblings +
  plot_transport_means + plot_wkly_study_hours
) + plot_layout(ncol = 2, heights = c(1, 1, 1, 1, 1)) # Adjust the number of rows to your needs

# Save as a larger plot with better dimensions
ggsave("combined_plot.png", combined_plot, width = 12, height = 18) # Adjust height/width

# Display the adjusted plot
print(combined_plot)


```
visualize correlations
```{r}
pairs(math_df_numeric)
png("correlation_plot.png", width = 6, height = 4, units = "in", res = 150)
corrplot(cor(math_df_numeric), type = "upper", diag = FALSE)
```
looks like math score is correlated with gender, ethnic group, parent education, lunch type, and test prep. Also slightly correlated with nr siblings and weekly study hours

correlation between predictors:
marital status & is first child
parent education & nr siblings, transportation means
test prep & is first child, nr siblings, transportation means, weekly study hours
is first child & nr siblings


Use automatic procedures to find a ‘best subset’ of the full model for predicting math scores.

```{r}
# Full model
full_model <- lm(math_score ~ ., data = math_df)

summary(full_model)
```

```{r}
# Backward elimination
backward_model <- step(full_model, direction = "backward")

summary(backward_model)

model_summary = summary(backward_model)

coeff_table <- model_summary$coefficients

# Filter for significant terms (p-value < 0.05)
significant_terms <- coeff_table[coeff_table[, "Pr(>|t|)"] < 0.05, ]

# Create a data frame for better readability
significant_terms_df <- data.frame(
  Term = rownames(significant_terms),
  Coefficient = significant_terms[, "Estimate"],
  P_Value = significant_terms[, "Pr(>|t|)"]
)

# Print the significant terms
knitr::kable(significant_terms_df)

```
forward selection
```{r}
# Null model (no predictors)
null_model <- lm(math_score ~ 1, data = math_df)

# Forward selection
forward_model <- step(null_model, direction = "forward", scope = formula(full_model))

# Summary of the final model
summary(forward_model)

model_summary = summary(forward_model)

coeff_table <- model_summary$coefficients

# Filter for significant terms (p-value < 0.05)
significant_terms <- coeff_table[coeff_table[, "Pr(>|t|)"] < 0.05, ]

# Create a data frame for better readability
significant_terms_df <- data.frame(
  Term = rownames(significant_terms),
  Coefficient = significant_terms[, "Estimate"],
  P_Value = significant_terms[, "Pr(>|t|)"]
)

# Print the significant terms
knitr::kable(significant_terms_df)
```
stepwise
```{r}
# Stepwise regression
stepwise_model <- step(full_model, direction = "both")

# Summary of the final model
summary(stepwise_model)

model_summary = summary(stepwise_model)

coeff_table <- model_summary$coefficients

# Filter for significant terms (p-value < 0.05)
significant_terms <- coeff_table[coeff_table[, "Pr(>|t|)"] < 0.05, ]

# Create a data frame for better readability
significant_terms_df <- data.frame(
  Term = rownames(significant_terms),
  Coefficient = significant_terms[, "Estimate"],
  P_Value = significant_terms[, "Pr(>|t|)"]
)

# Print the significant terms
knitr::kable(significant_terms_df)
```

* Do the procedures generate the same model?

yes! significant predictors are (7):

* Gender
* ethnic_group
* parent_educ
* lunch_type
* test_prep
* parent_marital_status
* wkly_study_hours

Since multi-colinearity can make a predictor appear less significant, we should check for it.
```{r}
vif(lm(math_score ~ ., data = math_df))
```
None of the VIFs are larger than 5, so multi-colinearity is likely not a concern.

Use criterion-based procedures to guide your selection of the ‘best subset’. Summarize your results (tabular or graphical).

```{r}
mat = as.matrix(math_df_numeric)
# Printing the 2 best models of each size, using the Cp criterion:
leaps(x = mat[,2:12], y = mat[,1], nbest = 2, method = "Cp")

# Printing the 2 best models of each size, using the adjusted R^2 criterion:
leaps(x = mat[,2:12], y = mat[,1], nbest = 2, method = "adjr2")

# Function regsubsets() performs a subset selection by identifying the "best" model that contains
# a certain number of predictors. By default "best" is chosen using SSE/RSS (smaller is better)
b = regsubsets(math_score ~ ., data = math_df_numeric)
rs = summary(b)

# plot of Cp and Adj-R2 as functions of parameters
par(mfrow=c(1,2))

plot(2:9, rs$cp, xlab="No of parameters", ylab="Cp Statistic")
abline(0,1)

plot(2:9, rs$adjr2, xlab="No of parameters", ylab="Adj R2")
```
Cp is close to p at 7 predictors; Adjusted R square increases rapidly from 2-5 predictors and peaks at 8 predictors. Overall it seems like 7 or 8 would be a good choice.

Use the LASSO method to perform variable selection. Make sure you choose the “best lambda” to use and show how you determined this.
```{r}
# Separate the response variable (life_exp) and predictors
y <- math_df_numeric$math_score
X <- as.matrix(math_df_numeric[, -1])  # Exclude the response variable

# Fit LASSO model with cross-validation
lasso_cv <- cv.glmnet(X, y, alpha = 1)  # alpha = 1 for LASSO (L2 regularization)

# Plot the cross-validation results
plot(lasso_cv)

# Best lambda (lambda.min is the one that minimizes cross-validation error)
best_lambda <- lasso_cv$lambda.min
best_lambda

# Coefficients at the best lambda
lasso_coefs <- coef(lasso_cv, s = "lambda.min")
print(lasso_coefs)
```
The LASSO model narrows down to 9 predictors:
`gender`
`ethnic_group`
`parent_educ`
`lunch_type`
`test_prep`
`parent_marital_status`
`wkly_study_hours`
`is_first_child`
`nr_siblings`

# evaluating candidate models
## MLR assumptions
### 7-predictor model
* how are the models in terms of assumptions?
```{r}
model_1 = lm(math_score ~ gender + ethnic_group + parent_educ + 
    lunch_type + test_prep + parent_marital_status +  
    wkly_study_hours, data = math_df)

par(mfrow = c(2, 2))
plot(model_1)
```
QQ plot doesn't seem so normal, some deviation at large values. Let's try a transformation.
```{r}
math_df$math_score <- math_df$math_score + 1

boxcox(model_1, lambda = seq(-3, 3, by = 0.25))
```
looks like a power of 1.5 is recommended.
```{r}
math_df_tr = math_df %>% 
  mutate(sqrt_math_score = sqrt(math_score),
         log_math_score = log(math_score),
         ahalf_math_score = (math_score)^1.5)

model_1.5 = lm(sqrt_math_score ~ gender + ethnic_group + parent_educ + lunch_type
+ test_prep + parent_marital_status + wkly_study_hours, data = math_df_tr)

model_1.6 = lm(log_math_score ~ gender + ethnic_group + parent_educ + lunch_type
+ test_prep + parent_marital_status + wkly_study_hours, data = math_df_tr)

model_1.7 = lm(ahalf_math_score ~ gender + ethnic_group + parent_educ + lunch_type
+ test_prep + parent_marital_status + wkly_study_hours, data = math_df_tr)

plot(model_1)
plot(model_1.5)
plot(model_1.6)
plot(model_1.7)
```
The QQ plot did not get better with transformation.

### 9-predictor model
```{r}
model_3 = lm(math_score ~ gender + ethnic_group + parent_educ + lunch_type
+ test_prep + parent_marital_status + wkly_study_hours + is_first_child + nr_siblings,
data = math_df)

plot(model_3)
```
same issue with QQ plot

## Predictive power
### 7-predictor model
RMSE      Rsquared   MAE     
  13.71652  0.2769511  11.15795
```{r}
set.seed(1)
# Use 10-fold validation and create the training sets
train = trainControl(method = "cv", number = 10)

# Fit the 4-variables model
model_caret = train(math_score ~ gender + ethnic_group + parent_educ + 
    lunch_type + test_prep + parent_marital_status + 
    wkly_study_hours, data = math_df,
                   trControl = train,
                   method = 'lm',
                   na.action = na.pass)

model_caret$finalModel
print(model_caret)
```

### 9-predictor model
RMSE 13.68887
```{r}
train = trainControl(method = "cv", number = 10)

# Fit the 9-variables model
model_caret = train(math_score ~ gender + ethnic_group + parent_educ + lunch_type
+ test_prep + parent_marital_status + wkly_study_hours + is_first_child + nr_siblings,
data = math_df,
                   trControl = train,
                   method = 'lm',
                   na.action = na.pass)

model_caret$finalModel
print(model_caret)
```


Why does test preparation have a varying effect based on ethnic group or marital status?
Does the interaction between weekly study hours and marital status reflect broader socioeconomic patterns?

try visualization approach
Cox and Wermuth (1996) or Cox (1984) discussed some methods for detecting interactions. The problem is usually how general the interaction terms should be. Basically, we (a) fit (and test) all second-order interaction terms, one at a time, and (b) plot their corresponding p-values (i.e., the No. terms as a function of 1−𝑝). The idea is then to look if a certain number of interaction terms should be retained: Under the assumption that all interaction terms are null the distribution of the p-values should be uniform (or equivalently, the points on the scatterplot should be roughly distributed along a line passing through the origin).

fitting many (if not all) interactions might lead to overfitting, but it is also useless in a certain sense (some high-order interaction terms often have no sense at all). However, this has to do with interpretation, not detection of interactions
```{r}
# Fit all second-order interactions
fit_all_interactions <- lm(math_score ~ (gender + ethnic_group + parent_educ + 
    lunch_type + test_prep + parent_marital_status + nr_siblings + 
    wkly_study_hours)^2, data = math_df)

# Extract p-values for interaction terms
interaction_pvals = tidy(fit_all_interactions) %>%
  filter(str_detect(term, ":")) %>% # Keep only interaction terms
  dplyr::select(term, p.value) %>%
  arrange(p.value)

# Add rank for plotting
interaction_pvals <- interaction_pvals %>%
  mutate(rank = row_number())

# Plot p-values against 1 - p
ggplot(interaction_pvals, aes(x = rank, y = 1 - p.value)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, color = "red", linetype = "dashed") +
  labs(
    x = "Rank of interaction term",
    y = "1 - P-value",
    title = "Detection of Interaction Effects",
    subtitle = "Assuming null hypothesis, points should align along the diagonal"
  ) +
  theme_minimal()

# Print significant interaction terms
significant_interactions <- interaction_pvals %>%
  filter(p.value < 0.05)

print(significant_interactions)

```
There might be some significant interactions. So we explored all two way interactions and found significant ones as follows.
```{r}
interaction_model_8 = lm(math_score ~ (gender + ethnic_group + parent_educ + 
    lunch_type + test_prep + parent_marital_status + nr_siblings + 
    wkly_study_hours)^2, data = math_df)

model_summary = summary(interaction_model_8)

coeff_table <- model_summary$coefficients

# Filter for significant terms (p-value < 0.05)
significant_terms <- coeff_table[coeff_table[, "Pr(>|t|)"] < 0.05, ]

# Create a data frame for better readability
significant_terms_df <- data.frame(
  Term = rownames(significant_terms),
  Coefficient = significant_terms[, "Estimate"],
  P_Value = significant_terms[, "Pr(>|t|)"]
)

# Print the significant terms
knitr::kable(significant_terms_df)
  
```
We add the significant ones one by one, keep the significant ones here. Thought: because we have 8 predictors, we will have 28 possible two way interactions. The multiple comparisons might lead to significance due to chance.
```{r}
interaction_model_1.2 = lm(math_score ~ gender + ethnic_group + parent_educ + 
    lunch_type + test_prep + parent_marital_status + nr_siblings + 
    wkly_study_hours + 
      parent_educ:parent_marital_status + 
      test_prep:parent_marital_status, data = math_df)

summary(interaction_model_1.2)
```
The adjusted r squared is only .02 higher than before adding the interaction terms.

Let's look at the model's predictive ability using a 10-fold cv

for interaction_model_1.2 is
RMSE      Rsquared     
  13.77241  0.2788751
vs the model without interactions:
RMSE      Rsquared        
  13.71652  0.2769511
  
The large model predicts no better than the small model. For simplicity, we choose the small model.
```{r}
# Use 10-fold validation and create the training sets
train = trainControl(method = "cv", number = 10)

# Fit the 4-variables model
model_caret = train(math_score ~ gender + ethnic_group + parent_educ + 
    lunch_type + test_prep + parent_marital_status + nr_siblings + 
    wkly_study_hours + parent_educ:parent_marital_status + test_prep:parent_marital_status, data = math_df,
                   trControl = train,
                   method = 'lm',
                   na.action = na.pass)

model_caret$finalModel
print(model_caret)
```

```{r}
model_1 = lm(math_score ~ gender + ethnic_group + parent_educ + 
    lunch_type + test_prep + parent_marital_status + nr_siblings + 
    wkly_study_hours, data = math_df)

model_summary = summary(model_1)

coeff_table <- model_summary$coefficients

# Filter for significant terms (p-value < 0.05)
significant_terms <- coeff_table[coeff_table[, "Pr(>|t|)"] < 0.05, ]

# Create a data frame for better readability
significant_terms_df <- data.frame(
  Term = rownames(significant_terms),
  Coefficient = significant_terms[, "Estimate"],
  P_Value = significant_terms[, "Pr(>|t|)"]
)

# Print the significant terms
knitr::kable(significant_terms_df)
```

# 3. Are the optimal prediction models similar or different across the three test scores? Is it possible to leverage one score as the auxiliary information to learn the model for another score (still its model against variables 1-11) better?

```{r}
scores_model = lm(math_score ~ gender + ethnic_group + parent_educ + 
    lunch_type + test_prep + parent_marital_status + nr_siblings + wkly_study_hours + 
      reading_score+
      writing_score, data = score_df)

model_summary = summary(scores_model)

coeff_table <- model_summary$coefficients

# Filter for significant terms (p-value < 0.05)
significant_terms <- coeff_table[coeff_table[, "Pr(>|t|)"] < 0.05, ]

# Create a data frame for better readability
significant_terms_df <- data.frame(
  Term = rownames(significant_terms),
  Coefficient = significant_terms[, "Estimate"],
  P_Value = significant_terms[, "Pr(>|t|)"]
)

# Print the significant terms
knitr::kable(significant_terms_df)
```
This model is doing really well, explaining about 89 percent of the variance in math scores.

a cv show significantly improved performance
RMSE     Rsquared   MAE     
  5.49093  0.8855928  4.407864
```{r}
# Use 10-fold validation and create the training sets
train = trainControl(method = "cv", number = 10)

# Fit the 4-variables model
model_caret = train(math_score ~ gender + ethnic_group + parent_educ + 
    lunch_type + test_prep + parent_marital_status + nr_siblings + wkly_study_hours + 
      reading_score + writing_score, data = score_df,
                   trControl = train,
                   method = 'lm',
                   na.action = na.pass)

model_caret$finalModel
print(model_caret)
```

So yes, we definitely should add the other two scores to predict math scores in addition to the social and personal factors.
